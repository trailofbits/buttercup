# Global values available to all charts
global:
  # Set environment: "minikube" or "aks"
  environment: "minikube"
  logMaxLineLength: 10240
  orchestratorImage:
    repository: ghcr.io/trailofbits/buttercup/buttercup-orchestrator
    tag: "main"
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]
  fuzzerImage:
    repository: ghcr.io/trailofbits/buttercup/buttercup-fuzzer
    tag: "main"
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]
  seedGenImage:
    repository: ghcr.io/trailofbits/buttercup/buttercup-seed-gen
    tag: "main"
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]
  patcherImage:
    repository: ghcr.io/trailofbits/buttercup/buttercup-patcher
    tag: "main"
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]
  programModelImage:
    repository: ghcr.io/trailofbits/buttercup/buttercup-program-model
    tag: "main"
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]
  competitionApiImage:
    repository: ghcr.io/tob-challenges/example-crs-architecture/competition-test-api
    tag: v1.4-rc1
    pullPolicy: Always
    pullSecrets: ["ghcr-auth"]

  # Default sample size for the coverage bot
  coverageBot:
    sampleSize: 200

  # Max PoV size limit
  maxPovSize: 2097152  # 2 MiB

  # Langfuse global configuration for all subcharts
  langfuse:
    enabled: false
    host: "https://cloud.langfuse.com"
    publicKey: "pk-lf-..."  # Replace with your actual public key
    secretKey: "sk-lf-..."  # Replace with your actual secret key
  crs:
    api_key_id: 515cc8a0-3019-4c9f-8c1c-72d0b54ae561
    api_key_token: VGuAC8axfOnFXKBB7irpNDOKcDjOlnyB
    api_key_token_hash: "$argon2id$v=19$m=65536,t=3,p=4$Dg1v6NPGTyXPoOPF4ozD5A$wa/85ttk17bBsIASSwdR/uGz5UKN/bZuu4wu+JIy1iA"
    # api_url: "https://api.tail7e9b4c.ts.net"
    hostname: "buttercup-crs"
    competition_api_key_id: 11111111-1111-1111-1111-111111111111
    competition_api_key_token: secret
    # competition_api_url: "https://api.tail7e9b4c.ts.net"
  otel:
    endpoint: "https://127.0.0.1:4318" # TODO: Change to the actual endpoint
    token: "token"
    protocol: "grpc"
  queueTimeouts:
    buildTaskTimeoutMs: 120000
    buildOutputTaskTimeoutMs: 120000
    downloadTaskTimeoutMs: 120000
    readyTaskTimeoutMs: 120000
    deleteTaskTimeoutMs: 120000
    crashTaskTimeoutMs: 120000
    patchTaskTimeoutMs: 120000
    confirmedVulnerabilitiesTaskTimeoutMs: 120000
    indexTaskTimeoutMs: 120000
    indexOutputTaskTimeoutMs: 120000
    tracedVulnerabilitiesTaskTimeoutMs: 120000
  # Node-local storage configuration to be accessible by all subcharts
  volumes:
    nodeLocal:
      enabled: true
      hostPath: /node_data_storage
      type: DirectoryOrCreate
      mountPath: /node_data

# Volume configurations
volumes:
  tasks_storage:
    enabled: true
    # When using AKS, specify the appropriate storage class for ReadWriteMany support
    # For example: "azurefile-csi-nfs-${BUTTERCUP_NAMESPACE}"
    storageClass: ""  # Empty string will use cluster default
    size: "5Gi"
    accessMode: ReadWriteMany
  crs_scratch:
    enabled: true
    # When using AKS, specify the appropriate storage class for ReadWriteMany support
    storageClass: ""  # Empty string will use cluster default
    size: "10Gi"
    accessMode: ReadWriteMany


# Dind daemon chart configuration
dind-daemon:
  enabled: true
  resources:
    limits:
      cpu: 4000m
      memory: 16Gi
    requests:
      cpu: 500m
      memory: 1Gi

# Service-specific configurations
redis:
  auth:
    enabled: false  # Disable authentication for simplicity
  architecture: standalone  # Use single instance instead of cluster
  master:
    persistence:
      enabled: true
      size: 8Gi # 8GB of storage for redis, how much is needed?
      storageClass: managed-premium

    configuration: |-
      appendonly yes
      appendfsync everysec
      no-appendfsync-on-rewrite no
      auto-aof-rewrite-percentage 100
      auto-aof-rewrite-min-size 64mb
    resources:
      limits:
        cpu: 250m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 256Mi
  # Add Docker registry authentication
  global:
    imagePullSecrets:
      - ghcr-auth
      - docker-auth

task-server:
  enabled: true
  # API authentication is configured with the following credentials:
  # API Key ID: 515cc8a0-3019-4c9f-8c1c-72d0b54ae561
  # API Token: VGuAC8axfOnFXKBB7irpNDOKcDjOlnyB (for client usage)
  # These values are set in the secret/config-map in config.yaml

task-downloader:
  enabled: true
  
scheduler:
  enabled: true

program-model:
  enabled: true

build-bot:
  replicaCount: 4
  enabled: true
  timer: 5000
  logLevel: "DEBUG"

fuzzer-bot:
  enabled: true

coverage-bot:
  enabled: true

tracer-bot:
  enabled: true

seed-gen:
  enabled: true
  logMaxLineLength: 10240
  maxCorpusSeedSize: 65536  # 64 KiB

patcher:
  enabled: true

competition-api:
  enabled: true

# Registry cache configuration
registry-cache:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi

# Scratch cleaner configuration
scratch-cleaner:
  enabled: true
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 256Mi

# LiteLLM configuration
litellm-helm:
  # Override the name to ensure it matches our references
  nameOverride: "litellm"
  
  # Number of workers for the LiteLLM service
  replicaCount: 1
  
  # Set the service port
  service:
    port: 4000
  
  # Image pull secrets for all containers (main, init, and PostgreSQL)
  imagePullSecrets:
    - name: ghcr-auth
    - name: docker-auth
  
  # Configure the built-in PostgreSQL with our desired settings
  postgresql:
    enabled: true
    image:
      tag: "17.2.0"
    auth:
      username: litellm_user
      password: litellm_password11
      database: litellm
      enablePostgresUser: true
      postgresPassword: litellm_password11
    primary:
      resources:
        limits:
          cpu: 1000m
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 256Mi
    readReplicas:
      replicaCount: 0
  
  # Configure database connection properly
  db:
    deployStandalone: true
    endpoint: "buttercup-litellm-postgresql"
    database: "litellm"
    url: "postgresql://litellm_user:litellm_password11@buttercup-litellm-postgresql:5432/litellm"
  
  # Environment variables - only keep DATABASE_URL as backup
  envVars:
    DATABASE_URL: "postgresql://litellm_user:litellm_password11@buttercup-litellm-postgresql:5432/litellm"
    # API keys are now provided through Kubernetes secrets
  
  # Use environment secrets for API keys
  # For subcharts, we need to use a static name that will be templated at the parent level
  environmentSecrets:
    - "buttercup-litellm-api-secrets"
  
  # Direct inclusion of the litellm_config.yaml content
  proxy_config:
    model_list:
      - model_name: azure-gpt-4o
        litellm_params:
          model: azure/gpt-4o
          api_base: os.environ/AZURE_API_BASE
          api_key: os.environ/AZURE_API_KEY
          tpm: 30000
          rpm: 1800

      - model_name: azure-gpt-4o-mini
        litellm_params:
          model: azure/gpt-4o-mini
          api_base: os.environ/AZURE_API_BASE
          api_key: os.environ/AZURE_API_KEY
          tpm: 50000
          rpm: 5000

      - model_name: azure-o3-mini
        litellm_params:
          model: azure/o3-mini
          api_base: os.environ/AZURE_API_BASE
          api_key: os.environ/AZURE_API_KEY
          api_version: 2024-12-01-preview
          tpm: 50000
          rpm: 50

      - model_name: azure-o1
        litellm_params:
          model: azure/o1
          api_base: os.environ/AZURE_API_BASE
          api_key: os.environ/AZURE_API_KEY
          api_version: 2024-12-01-preview
          tpm: 30000
          rpm: 50

      - model_name: openai-gpt-4o
        litellm_params:
          model: openai/gpt-4o
          api_key: os.environ/OPENAI_API_KEY
          tpm: 30000000
          rpm: 10000

      - model_name: openai-gpt-4o-mini
        litellm_params:
          model: openai/gpt-4o-mini
          api_key: os.environ/OPENAI_API_KEY
          tpm: 150000000
          rpm: 30000

      - model_name: openai-o3-mini
        litellm_params:
          model: openai/o3-mini
          api_key: os.environ/OPENAI_API_KEY
          tpm: 150000000
          rpm: 30000

      - model_name: openai-o1
        litellm_params:
          model: openai/o1
          api_key: os.environ/OPENAI_API_KEY
          tpm: 30000000
          rpm: 10000

      - model_name: openai-o3
        litellm_params:
          model: openai/o3
          api_key: os.environ/OPENAI_API_KEY
          tpm: 30000000
          rpm: 10000

      - model_name: openai-gpt-4.1
        litellm_params:
          model: openai/gpt-4.1
          api_key: os.environ/OPENAI_API_KEY
          tpm: 30000000
          rpm: 10000

      - model_name: openai-gpt-4.1-mini
        litellm_params:
          model: openai/gpt-4.1-mini
          api_key: os.environ/OPENAI_API_KEY
          tpm: 150000000
          rpm: 30000

      - model_name: openai-gpt-4.1-nano
        litellm_params:
          model: openai/gpt-4.1-nano
          api_key: os.environ/OPENAI_API_KEY
          tpm: 150000000
          rpm: 30000

      - model_name: claude-3.5-sonnet
        litellm_params:
          model: anthropic/claude-3-5-sonnet-20241022
          api_key: os.environ/ANTHROPIC_API_KEY
          tpm: 600000
          rpm: 6250

      - model_name: claude-3.7-sonnet
        litellm_params:
          model: anthropic/claude-3-7-sonnet-20250219
          api_key: os.environ/ANTHROPIC_API_KEY
          tpm: 175000
          rpm: 6250

      - model_name: claude-4-sonnet
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: os.environ/ANTHROPIC_API_KEY
          tpm: 1775000

    general_settings:
      master_key: os.environ/BUTTERCUP_LITELLM_KEY
      database_url: "postgresql://litellm_user:litellm_password11@buttercup-postgresql:5432/litellm"
      store_model_in_db: true

    litellm_settings:
      max_budget: 100
      budget_duration: 30d
      # store_prompts_in_spend_logs: true

restartPolicy: Always

