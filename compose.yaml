services:
  # ugh the only solution i can come up with is a dind instance paired with every single build-bot we will need like k8s groups or something
  dind:
    expose:
      - "2375"
    image: docker:24-dind
    command: ["dockerd", "-H", "tcp://0.0.0.0:2375", "--tls=false", "--storage-driver=overlay2"]
    restart: always
    privileged: true  # This must run with privlege to support nested virtualization within the public Linux CP for `virtme-ng`
    environment:
      - DOCKER_TLS_CERTDIR  # intentionally blank to optimize runtime
    volumes:
      - type: bind
        source: ./crs_scratch
        target: /crs_scratch

  redis:
    image: redis:7.4.2
    restart: always
    ports:
      - '127.0.0.1:6379:6379'
    # NOTE: Do not use the configuration file for now, so every time you restart
    # the container, it will reset the redis instance
    # command: redis-server /mount/redis.conf
    # volumes:
      # - cache:/data
      # - ./redis:/mount
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 1s
      timeout: 3s
      retries: 5

  stimulate-fuzzer-test:
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    command: /fuzzer/.venv/bin/python -m buttercup.fuzzing_infra.stimulate_build_bot --build_type fuzzer --ossfuzz /crs_scratch/oss-fuzz --engine libfuzzer --sanitizer address --target_package ${TARGET_PACKAGE} --redis_url redis://redis:6379 --source_path /crs_scratch/${TARGET_PACKAGE} --task_id 123e4567-e89b-12d3-a456-426614174000
    profiles:
      - fuzzer-test
    restart: no
    volumes:
      - ./crs_scratch:/crs_scratch
    depends_on:
      redis:
        condition: service_healthy
      build-bot:
        condition: service_started
      fuzzer-bot:
        condition: service_started
      orchestrator-sim:
        condition: service_started
      coverage-bot:
        condition: service_started
      

  orchestrator-sim:
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    command: /fuzzer/.venv/bin/python -m buttercup.fuzzing_infra.orchestrator --redis_url redis://redis:6379
    profiles:
      - fuzzer-test
    volumes:
      - ./crs_scratch:/crs_scratch
    depends_on:
      redis:
        condition: service_healthy

  graphdb:
    image: janusgraph/janusgraph:1.2.0-20250126-150810.46a971a
    environment:
      - janusgraph.storage.backend=cql
      - janusgraph.storage.hostname=graphdb-storage
      - janusgraph.storage.cql.keyspace=janusgraph
      - janusgraph.schema.init.strategy=json
      - janusgraph.schema.init.json.file=/opt/janusgraph/conf/schema.json
      - _JAVA_OPTIONS=-Xmx2g -Xms1g
    profiles:
      - program-model-test
    healthcheck:
      test: ["CMD-SHELL", "bin/gremlin.sh", "-e", "scripts/remote-connect.groovy"]
      interval: 10s
      timeout: 60s
      retries: 4
    ports:
      - "127.0.0.1:8182:8182"
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./program-model/conf/janusgraph-server.yaml:/opt/janusgraph/conf/janusgraph-server.yaml
      - ./program-model/conf/schema.json:/opt/janusgraph/conf/schema.json
    depends_on:
      graphdb-storage:
        condition: service_healthy

  graphdb-storage:
    image: cassandra:4.0.6
    environment:
      - CASSANDRA_START_RPC=true
    ports:
      - "127.0.0.1:9042:9042"
      - "127.0.0.1:9160:9160"
    volumes:
      - graphdb_data:/var/lib/cassandra/data
    profiles:
      - program-model-test
    healthcheck:
      test: ["CMD-SHELL", "nodetool", "status"]
      interval: 10s
      timeout: 60s
      retries: 4

  program-model:
    build:
      context: ./
      dockerfile: ./program-model/Dockerfile
    command: [
      "buttercup-program-model",
      "--log_level", "debug",
      "serve",
      "--wdir", "/crs_scratch",
      "--sleep_time", "5",
      "--redis_url", "redis://redis:6379",
    ]
    env_file: env.dev.compose
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./tasks_storage:/tasks_storage
    depends_on:
      redis:
        condition: service_healthy
      dind:
        condition: service_started
      # graphdb:
      #   condition: service_healthy

  coverage-bot:
    command: ["buttercup-coverage-bot", "--wdir", "/crs_scratch", "--redis_url", "redis://redis:6379"]
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./tasks_storage:/tasks_storage
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy
      dind:
        condition: service_started

  build-bot:
    command: ["buttercup-fuzzer-builder", "--wdir", "/crs_scratch", "--redis_url", "redis://redis:6379", "--timer", "5000"]
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./tasks_storage:/tasks_storage
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy
      dind:
        condition: service_started

  tracer-bot:
    command: ["buttercup-tracer-bot", "--wdir", "/crs_scratch", "--redis_url", "redis://redis:6379"]
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./tasks_storage:/tasks_storage
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy
      dind:
        condition: service_started

  fuzzer-bot:
    build:
      context: ./
      dockerfile: ./fuzzer/dockerfiles/runner_image.Dockerfile
    command: ["buttercup-fuzzer", "--wdir", "/crs_scratch", "--redis_url", "redis://redis:6379", "--timeout", "900", "--timer", "5000"]
    env_file: env.dev.compose
    volumes:
      - ./crs_scratch:/crs_scratch
    depends_on:
      redis:
        condition: service_healthy

  task-downloader:
    build:
      context: .
      dockerfile: ./orchestrator/Dockerfile
    command: ["buttercup-task-downloader", "serve"]
    volumes:
      - ./tasks_storage:/tasks_storage
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy

  task-server:
    build:
      context: .
      dockerfile: ./orchestrator/Dockerfile
    command: ["buttercup-task-server"]
    ports:
      - "127.0.0.1:8000:8000"
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy

  scheduler:
    build:
      context: .
      dockerfile: ./orchestrator/Dockerfile
    command: ["buttercup-scheduler", "serve"]
    volumes:
      - ./crs_scratch:/crs_scratch
      - ./tasks_storage:/tasks_storage
    env_file: env.dev.compose
    depends_on:
      redis:
        condition: service_healthy

  seed-gen:
    build:
      context: .
      dockerfile: ./seed-gen/Dockerfile
    # TODO: Update seed-gen sleep once service is doing useful work
    command: ["seed-gen", "server", "--redis_url", "redis://redis:6379", "--wdir", "/crs_scratch", "--sleep", "60"]
    volumes:
      - ./crs_scratch:/crs_scratch
    env_file: env.dev.compose
    environment:
      - LOG_LEVEL=DEBUG
      - LANGFUSE_HOST=${LANGFUSE_HOST}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
    depends_on:
      redis:
        condition: service_healthy
      litellm:
        condition: service_started
      dind:
        condition: service_started

  patcher:
    build:
      context: .
      dockerfile: ./patcher/Dockerfile
    command: ["buttercup-patcher", "serve"]
    volumes:
      - ./crs_scratch:/crs_scratch
    env_file: env.dev.compose
    environment:
      - LANGFUSE_HOST=${LANGFUSE_HOST}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
    depends_on:
      redis:
        condition: service_healthy
      dind:
        condition: service_started

  litellm:
    image: ghcr.io/berriai/litellm:litellm_stable_release_branch-v1.57.8-stable
    configs:
      - source: litellm_config
        target: /app/config.yaml
    env_file: .env
    ports:
      - "127.0.0.1:8080:8080"
    environment:
        DATABASE_URL: "postgresql://litellm_user:litellm_password11@litellm-db:5432/litellm"

    # note: litellm recommends 1 worker for kubernetes: https://docs.litellm.ai/docs/proxy/prod
    command: ["--config", "/app/config.yaml", "--port", "8080", "--num_workers", "8"]
    depends_on:
      litellm-db:
        condition: service_healthy

  litellm-db:
    image: postgres:17.2
    restart: always
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm_user
      POSTGRES_PASSWORD: litellm_password11
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 5s
      timeout: 5s
      retries: 10

  competition-api:
    image: ghcr.io/aixcc-finals/example-crs-architecture/example-competition-api:v0.1

configs:
  litellm_config:
    file: ./litellm/litellm_config.yaml

volumes:
  cache:
    driver: local
  graphdb_data:
    driver: local
