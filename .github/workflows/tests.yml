name: Unit tests

on:
  push:
    branches:
      - main
    # Always run full test suite on main branch
  pull_request:
    paths:
      # Run if any Python code changes
      - '**/*.py'
      # Run if any test files change
      - '**/tests/**'
      - '**/test_*.py'
      - '**/*_test.py'
      # Run if any dependencies change
      - '**/pyproject.toml'
      - '**/uv.lock'
      - '**/requirements*.txt'
      # Run if build/CI configuration changes
      - '**/Makefile'
      - '.github/workflows/tests.yml'
      - '.github/workflows/lint.yml'
      # Run if protobuf definitions change
      - '**/*.proto'
      - 'protoc.sh'
      # Run if Docker configuration changes (affects test environment)
      - '**/Dockerfile'
      - '**/.dockerignore'
      # Run if test data changes
      - '**/testdata/**'
      - '**/fixtures/**'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    strategy:
      fail-fast: false  # Continue running other components even if one fails
      matrix:
        component:
          [common, orchestrator, program-model, seed-gen, patcher]
        python:
          - "3.12"
        # TODO: Add integration tests back in
        # include:
        #   - component: common
        #     pytest_args: "--runintegration"
        #   - component: program-model
        #     pytest_args: "--runintegration"

    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: true

      - name: Install uv
        uses: astral-sh/setup-uv@v6

      - name: Setup uv cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.local/share/uv
          key: ${{ runner.os }}-uv-${{ matrix.component }}-${{ hashFiles(format('{0}/uv.lock', matrix.component)) }}
          restore-keys: |
            ${{ runner.os }}-uv-${{ matrix.component }}-
            ${{ runner.os }}-uv-

      - name: Download Wasm runtime
        run: wget https://github.com/vmware-labs/webassembly-language-runtimes/releases/download/python%2F3.12.0%2B20231211-040d5a6/python-3.12.0.wasm
        if: matrix.component == 'seed-gen'
        working-directory: seed-gen

      - name: Install dependencies for program-model and seed-gen
        if: matrix.component == 'program-model' || matrix.component == 'seed-gen'
        run: |
          sudo apt-get update
          sudo apt-get install -y codequery ripgrep
          make install-cscope

      - name: Install minimal dependencies
        if: matrix.component != 'program-model' && matrix.component != 'seed-gen'
        run: |
          sudo apt-get update
          sudo apt-get install -y ripgrep

      - name: Prepare environment
        run: |
          export DEBIAN_FRONTEND=noninteractive
          sudo apt-get update
          sudo mkdir -p /crs_scratch
          sudo chmod -R 777 /crs_scratch

      - name: Setup ${{ matrix.component }} component
        run: |
          uv sync --all-extras --frozen
          # Install test reporting tools if not already present
          uv pip install pytest-html pytest-cov
        working-directory: ${{ matrix.component }}

      - name: Run tests on ${{ matrix.component }} component
        run: |
          # Convert component name to Python module name (e.g., seed-gen -> seed_gen)
          PYTHON_MODULE=$(echo "${{ matrix.component }}" | tr '-' '_')
          uv run --frozen pytest -svv ${{ matrix.pytest_args }} \
            --junit-xml=test-results.xml \
            --html=test-report.html \
            --self-contained-html \
            --cov=${PYTHON_MODULE} \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term
        env:
          PYTHON_WASM_BUILD_PATH: "python-3.12.0.wasm"
        working-directory: ${{ matrix.component }}

      - name: Generate test summary
        if: always()
        run: |
          echo "### Test Results: ${{ matrix.component }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f ${{ matrix.component }}/test-results.xml ]; then
            python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('${{ matrix.component }}/test-results.xml')
          root = tree.getroot()
          tests = root.get('tests', '0')
          failures = root.get('failures', '0')
          errors = root.get('errors', '0')
          skipped = root.get('skipped', '0')
          time = root.get('time', '0')
          print(f'- **Total Tests**: {tests}')
          print(f'- **Passed**: {int(tests) - int(failures) - int(errors) - int(skipped)}')
          print(f'- **Failed**: {failures}')
          print(f'- **Errors**: {errors}')
          print(f'- **Skipped**: {skipped}')
          print(f'- **Duration**: {float(time):.2f}s')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "No test results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.component }}-py${{ matrix.python }}
          path: |
            ${{ matrix.component }}/test-results.xml
            ${{ matrix.component }}/test-report.html
            ${{ matrix.component }}/coverage.xml
            ${{ matrix.component }}/htmlcov/
          retention-days: 30

      - name: Upload coverage reports
        if: always()
        uses: codecov/codecov-action@v4
        with:
          file: ${{ matrix.component }}/coverage.xml
          flags: ${{ matrix.component }}
          name: ${{ matrix.component }}-py${{ matrix.python }}
          fail_ci_if_error: false  # Don't fail if codecov is down

  # Fuzzer tests are experimental and may fail
  # Run them separately so they don't block other components
  test-fuzzer-experimental:
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't fail the overall CI if fuzzer tests fail
    services:
      redis:
        image: redis
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: true

      - name: Install uv
        uses: astral-sh/setup-uv@v6

      - name: Setup uv cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.local/share/uv
          key: ${{ runner.os }}-uv-fuzzer-${{ hashFiles('fuzzer/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-fuzzer-
            ${{ runner.os }}-uv-

      - name: Install dependencies for fuzzer
        run: |
          sudo apt-get update
          sudo apt-get install -y ripgrep

      - name: Prepare environment
        run: |
          export DEBIAN_FRONTEND=noninteractive
          sudo apt-get update
          sudo mkdir -p /crs_scratch
          sudo chmod -R 777 /crs_scratch

      - name: Setup fuzzer component
        run: |
          uv sync --all-extras --frozen
          # Install test reporting tools if not already present
          uv pip install pytest-html pytest-cov
        working-directory: fuzzer

      - name: Run tests on fuzzer component (experimental)
        run: |
          # Convert component name to Python module name (for consistency)
          PYTHON_MODULE=$(echo "fuzzer" | tr '-' '_')
          uv run --frozen pytest -svv \
            --junit-xml=test-results.xml \
            --html=test-report.html \
            --self-contained-html \
            --cov=${PYTHON_MODULE} \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term || true  # Don't fail the job
        working-directory: fuzzer

      - name: Generate test summary for fuzzer
        if: always()
        run: |
          echo "### Test Results: fuzzer (experimental) ⚠️" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "*Note: Fuzzer tests are experimental and may fail*" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f fuzzer/test-results.xml ]; then
            python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('fuzzer/test-results.xml')
          root = tree.getroot()
          tests = root.get('tests', '0')
          failures = root.get('failures', '0')
          errors = root.get('errors', '0')
          skipped = root.get('skipped', '0')
          time = root.get('time', '0')
          print(f'- **Total Tests**: {tests}')
          print(f'- **Passed**: {int(tests) - int(failures) - int(errors) - int(skipped)}')
          print(f'- **Failed**: {failures}')
          print(f'- **Errors**: {errors}')
          print(f'- **Skipped**: {skipped}')
          print(f'- **Duration**: {float(time):.2f}s')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "No test results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload fuzzer test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-fuzzer-experimental
          path: |
            fuzzer/test-results.xml
            fuzzer/test-report.html
            fuzzer/coverage.xml
            fuzzer/htmlcov/
          retention-days: 30
